{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'sentence1': [' I am going to India ', 'I will be eating coffee' ], 'sentence2': ['I am going to bharat', 'I will be drinking coffee']}\n",
    "data = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashankhalo7/.local/lib/python3.5/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['wmd'] = data.apply(lambda x: wmd(x['sentence1'], x['sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 14.34it/s]\n",
      "2it [00:00, 1488.93it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence1_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "#Word2Vec Features\n",
    "for i, q in tqdm(enumerate(data.sentence1.values)):\n",
    "    sentence1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "sentence2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data.sentence2.values)):\n",
    "    sentence2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]\n",
    "data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(sentence1_vectors), np.nan_to_num(sentence2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>wmd</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am going to India</td>\n",
       "      <td>I am going to bharat</td>\n",
       "      <td>1.575631</td>\n",
       "      <td>0.315864</td>\n",
       "      <td>11.185450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>158.725751</td>\n",
       "      <td>0.794813</td>\n",
       "      <td>0.356511</td>\n",
       "      <td>0.441748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will be eating coffee</td>\n",
       "      <td>I will be drinking coffee</td>\n",
       "      <td>1.616904</td>\n",
       "      <td>0.221144</td>\n",
       "      <td>9.256322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.048932</td>\n",
       "      <td>0.665047</td>\n",
       "      <td>0.300408</td>\n",
       "      <td>0.355684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentence1                  sentence2       wmd  \\\n",
       "0     I am going to India        I am going to bharat  1.575631   \n",
       "1  I will be eating coffee  I will be drinking coffee  1.616904   \n",
       "\n",
       "   cosine_distance  cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "0         0.315864           11.185450               1.0         158.725751   \n",
       "1         0.221144            9.256322               1.0         146.048932   \n",
       "\n",
       "   euclidean_distance  minkowski_distance  braycurtis_distance  \n",
       "0            0.794813            0.356511             0.441748  \n",
       "1            0.665047            0.300408             0.355684  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
